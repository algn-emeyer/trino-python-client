# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import json
from textwrap import dedent
from typing import Any, Dict, List, Mapping, Optional, Sequence, Tuple, Union
from urllib.parse import unquote_plus

from sqlalchemy import exc, sql
from sqlalchemy.engine import Engine
from sqlalchemy.engine.base import Connection
from sqlalchemy.engine.default import DefaultDialect, DefaultExecutionContext
from sqlalchemy.engine.url import URL
from sqlalchemy.sql import sqltypes

from trino import dbapi as trino_dbapi
from trino import logging
from trino.auth import (
    BasicAuthentication,
    CertificateAuthentication,
    JWTAuthentication,
    OAuth2Authentication,
)
from trino.dbapi import Cursor
from trino.sqlalchemy import compiler, datatype, error
import os
import os.path
import time

from .datatype import JSONIndexType, JSONPathType

logger = logging.get_logger(__name__)

#
# Do we prefetch the DDL for all tables, columns, etc.?
#
colspecs = {
    sqltypes.JSON.JSONIndexType: JSONIndexType,
    sqltypes.JSON.JSONPathType: JSONPathType,
}


def is_file_older_than_x_days(file, days=1): 
    file_time = os.path.getmtime(file) 
    # Check against 24 hours 
    return ((time.time() - file_time) / 3600 > 24*days)

class TrinoDialect(DefaultDialect):
    def __init__(self,
                 json_serializer=None,
                 json_deserializer=None,
                 **kwargs):
        DefaultDialect.__init__(self, **kwargs)
        self._json_serializer = json_serializer
        self._json_deserializer = json_deserializer

    #
    # DDL Caching Facility
    prefetch_ddl = True
    ttl_days=7
    cache_columns = {}
    cache_catalogs = {}
    cache_schemas = {}
    cache_tables = {}
    cache_views = {}
    cache_view_definitions = {}
    cache_table_comments = {}

    name = "trino"
    driver = "rest"

    statement_compiler = compiler.TrinoSQLCompiler
    ddl_compiler = compiler.TrinoDDLCompiler
    type_compiler = compiler.TrinoTypeCompiler
    preparer = compiler.TrinoIdentifierPreparer

    # Data Type
    supports_native_enum = False
    supports_native_boolean = True
    supports_native_decimal = True

    # Column options
    supports_sequences = False
    supports_comments = True
    inline_comments = True
    supports_default_values = False

    # DDL
    supports_alter = True

    # DML
    # Queries of the form `INSERT () VALUES ()` is not supported by Trino.
    supports_empty_insert = False
    supports_multivalues_insert = True
    postfetch_lastrowid = False

    # Caching
    # Warnings are generated by SQLAlchmey if this flag is not explicitly set
    # and tests are needed before being enabled
    supports_statement_cache = False

    # Support proper ordering of CTEs in regard to an INSERT statement
    cte_follows_insert = True
    colspecs = colspecs

    @classmethod
    def dbapi(cls):
        """
        ref: https://www.python.org/dev/peps/pep-0249/#module-interface
        """
        return trino_dbapi

    @classmethod
    def import_dbapi(cls):
        """
        ref: https://www.python.org/dev/peps/pep-0249/#module-interface
        """
        return trino_dbapi

    def create_connect_args(self, url: URL) -> Tuple[Sequence[Any], Mapping[str, Any]]:
        args: Sequence[Any] = list()
        kwargs: Dict[str, Any] = dict(host=url.host)

        if url.port:
            kwargs["port"] = url.port

        db_parts = (url.database or "system").split("/")
        if len(db_parts) == 1:
            kwargs["catalog"] = unquote_plus(db_parts[0])
        elif len(db_parts) == 2:
            kwargs["catalog"] = unquote_plus(db_parts[0])
            kwargs["schema"] = unquote_plus(db_parts[1])
        else:
            raise ValueError(f"Unexpected database format {url.database}")

        if url.username:
            kwargs["user"] = unquote_plus(url.username)

        if url.password:
            if not url.username:
                raise ValueError("Username is required when specify password in connection URL")
            kwargs["http_scheme"] = "https"
            kwargs["auth"] = BasicAuthentication(unquote_plus(url.username), unquote_plus(url.password))

        if "access_token" in url.query:
            kwargs["http_scheme"] = "https"
            kwargs["auth"] = JWTAuthentication(unquote_plus(url.query["access_token"]))

        if "cert" and "key" in url.query:
            kwargs["http_scheme"] = "https"
            kwargs["auth"] = CertificateAuthentication(unquote_plus(url.query['cert']), unquote_plus(url.query['key']))

        if "externalAuthentication" in url.query:
            kwargs["http_scheme"] = "https"
            kwargs["auth"] = OAuth2Authentication()

        if "source" in url.query:
            kwargs["source"] = unquote_plus(url.query["source"])
        else:
            kwargs["source"] = "trino-sqlalchemy"

        if "session_properties" in url.query:
            kwargs["session_properties"] = json.loads(unquote_plus(url.query["session_properties"]))

        if "http_headers" in url.query:
            kwargs["http_headers"] = json.loads(unquote_plus(url.query["http_headers"]))

        if "extra_credential" in url.query:
            kwargs["extra_credential"] = [
                tuple(extra_credential) for extra_credential in json.loads(unquote_plus(url.query["extra_credential"]))
            ]

        if "client_tags" in url.query:
            kwargs["client_tags"] = json.loads(unquote_plus(url.query["client_tags"]))

        if "legacy_primitive_types" in url.query:
            kwargs["legacy_primitive_types"] = json.loads(unquote_plus(url.query["legacy_primitive_types"]))

        if "legacy_prepared_statements" in url.query:
            kwargs["legacy_prepared_statements"] = json.loads(unquote_plus(url.query["legacy_prepared_statements"]))

        if "verify" in url.query:
            kwargs["verify"] = json.loads(unquote_plus(url.query["verify"]))

        if "roles" in url.query:
            kwargs["roles"] = json.loads(url.query["roles"])

        if "ddlCache" in url.query:
            nbdays=json.loads(url.query["ddlCache"])
            print("trino:ddlCache=",nbdays)
            self.ttl_days = nbdays
            self.prefetch_ddl = True
            if (self.ttl_days<=0):
                self.ttl_days=7          

        return args, kwargs

    def get_columns(self, connection: Connection, table_name: str, schema: str = None, **kw) -> List[Dict[str, Any]]:
        if not self.has_table(connection, table_name, schema):
            raise exc.NoSuchTableError(f"schema={schema}, table={table_name}")
        return self._get_columns(connection, table_name, schema, **kw)

    def cache_load_columns(self, connection: Connection, schema: str = None):
        filepath=os.path.abspath("cache_columns.json")
        if (os.path.isfile(filepath) and not is_file_older_than_x_days(filepath, self.ttl_days)):
            with open(filepath, "r") as file:
                print("trino:load cache columns from file:",filepath)
                self.cache_columns = json.load(file)
        else:
            query = dedent(
                """
                SELECT
                    "table_name",
                    "column_name",
                    "data_type",
                    "column_default",
                    UPPER("is_nullable") AS "is_nullable"
                FROM "information_schema"."columns"
                WHERE "table_schema" = :schema
                ORDER BY "table_schema","table_name","ordinal_position" ASC
            """
            ).strip()
            res = connection.execute(sql.text(query), {"schema": schema})
            logger.debug("trino:fetch_cache_columns")
            for record in res:
                sn=schema+":"+record.table_name
                if sn not in self.cache_columns:
                    self.cache_columns[sn]={}
                self.cache_columns[sn][record.column_name]=dict(
                    name=record.column_name,
                    type=record.data_type,
                    nullable=record.is_nullable=="YES",
                    default=record.column_default
                    )
                print("trino:insert each column in cache:",sn,":",record.column_name,"->",self.cache_columns[sn])
            with open(filepath, "w") as file:
                json.dump(self.cache_columns, file)

    def _get_columns(self, connection: Connection, table_name: str, schema: str = None, **kw) -> List[Dict[str, Any]]:
        schema = schema or self._get_default_schema_name(connection)
        if self.prefetch_ddl:
            if self.cache_columns=={}:
                self.cache_load_columns(connection, schema)
            columns = []
            for record in self.cache_columns[schema+":"+table_name].values():
                ztype=datatype.parse_sqltype(record["type"])
                print("trino:find cache column:",schema,":",table_name,":",record,ztype)
                column = record
                column["type"]=ztype
                columns.append(column)   
            print("trino:columns=",columns)
        else:
            schema = schema or self._get_default_schema_name(connection)
            query = dedent(
                """
                SELECT
                    "column_name",
                    "data_type",
                    "column_default",
                    UPPER("is_nullable") AS "is_nullable"
                FROM "information_schema"."columns"
                WHERE "table_schema" = :schema
                AND "table_name" = :table
                ORDER BY "ordinal_position" ASC
            """
            ).strip()
            res = connection.execute(sql.text(query), {"schema": schema, "table": table_name})
            columns = []
            for record in res:
                column = dict(
                    name=record.column_name,
                    type=datatype.parse_sqltype(record.data_type),
                    nullable=record.is_nullable == "YES",
                    default=record.column_default,
                )
                columns.append(column)
        return columns

    def get_pk_constraint(self, connection: Connection, table_name: str, schema: str = None, **kw) -> Dict[str, Any]:
        """Trino has no support for primary keys. Returns a dummy"""
        return dict(name=None, constrained_columns=[])

    def get_primary_keys(self, connection: Connection, table_name: str, schema: str = None, **kw) -> List[str]:
        pk = self.get_pk_constraint(connection, table_name, schema)
        return pk.get("constrained_columns")  # type: ignore

    def get_foreign_keys(
        self, connection: Connection, table_name: str, schema: str = None, **kw
    ) -> List[Dict[str, Any]]:
        """Trino has no support for foreign keys. Returns an empty list."""
        return []

    def cache_load_catalogs(self, connection: Connection):
        query = dedent(
            """
            SELECT "table_cat"
            FROM "system"."jdbc"."catalogs"
            """
        ).strip()
        res = connection.execute(sql.text(query))
        logger.debug("trino:fetch_cache_catalogs")
        cache_catalogs=[];
        for record in res:
            cache_catalogs.append(record.table_cat)
        print("trino:insert each catalog in cache:->",cache_catalogs)

    def get_catalog_names(self, connection: Connection, **kw) -> List[str]:
        if self.prefetch_ddl:
            if self.cache_catalogs=={}:
                self.cache_load_catalogs(connection)
            catalogs = []
            for record in cache_catalogs.values():
                print("trino:find cache catalog:",record)
                catalog = record
                catalogs.append(catalog)   
            print("trino:catalogs=",catalogs)
            return[catalog.table_cat for catalog in catalogs]
        else:
            query = dedent(
                """
                SELECT "table_cat"
                FROM "system"."jdbc"."catalogs"
            """
            ).strip()
            res = connection.execute(sql.text(query))
            return [row.table_cat for row in res]

    def cache_load_schemas(self, connection: Connection):
        query = dedent(
            """
            SELECT "schema_name"
            FROM "information_schema"."schemata"
            """
        ).strip()
        res = connection.execute(sql.text(query))
        logger.debug("trino:fetch_cache_schemas")
        for record in res:
            self.cache_schemas[record.schema_name]=dict(
                schema_name=record.schema_name
                )
            print("trino:insert each schame in cache:->",self.cache_schemas[record.schema_name])

    def get_schema_names(self, connection: Connection, **kw) -> List[str]:
        if self.prefetch_ddl:
            if self.cache_schemas=={}:
                    self.cache_load_schemas(connection)
            schemas = []
            for record in self.cache_schemas.values():
                print("trino:find cache schemas:",record)
                schema = record
                schemas.append(schema)   
            print("trino:schemas=",schemas)
            return[schema.schema_name for schema in schemas]
        else:
            query = dedent(
                """
                SELECT "schema_name"
                FROM "information_schema"."schemata"
                """
            ).strip()
            res = connection.execute(sql.text(query))
            return [row.schema_name for row in res]

    def cache_load_tables(self, connection: Connection, schema: str = None):
        filepath=os.path.abspath("cache_tables.json")
        if (os.path.isfile(filepath) and not is_file_older_than_x_days(filepath, self.ttl_days)):
            print("trino:load cache tables from file:",filepath)
            with open(filepath, "r") as file:
                self.cache_tables = json.load(file)
        else:
            query = dedent(
                """
                SELECT "table_name"
                FROM "information_schema"."tables"
                WHERE "table_schema" = :schema
                AND "table_type" = 'BASE TABLE'
                ORDER BY "table_name"
                """
            ).strip()
            res = connection.execute(sql.text(query), {"schema": schema})
            logger.debug("trino:fetch_cache_tables")
            self.cache_tables[schema]=[]
            for record in res:
                self.cache_tables[schema].append(record.table_name)
            print("trino:insert each table in cache:->",self.cache_tables[schema])
            with open(filepath, "w") as file:  
                json.dump(self.cache_tables, file)

    def get_table_names(self, connection: Connection, schema: str = None, **kw) -> List[str]:
        schema = schema or self._get_default_schema_name(connection)
        if schema is None:
            raise exc.NoSuchTableError("schema is required")
        if self.prefetch_ddl:
            if self.cache_tables=={}:
                self.cache_load_tables(connection,schema)
            return self.cache_tables[schema]
        else:
            query = dedent(
                """
                SELECT "table_name"
                FROM "information_schema"."tables"
                WHERE "table_schema" = :schema
                AND "table_type" = 'BASE TABLE'
                ORDER BY "table_name"
                """
            ).strip()
            res = connection.execute(sql.text(query), {"schema": schema})
            return [row.table_name for row in res]

    def get_temp_table_names(self, connection: Connection, schema: str = None, **kw) -> List[str]:
        """Trino has no support for temporary tables. Returns an empty list."""
        return []

    def cache_load_views(self, connection: Connection, schema: str = None):
        filepath=os.path.abspath("cache_views.json")
        if (os.path.isfile(filepath) and not is_file_older_than_x_days(filepath, self.ttl_days)):
            with open(filepath, "r") as file:
                print("trino:load cache views from file:",filepath)
                self.cache_views = json.load(file)
        else:
            query = dedent(
                """
                SELECT "table_name"
                FROM "information_schema"."tables"
                WHERE "table_schema" = :schema
                AND "table_type" = 'VIEW'
                ORDER BY "table_name"
                """
            ).strip()
            res = connection.execute(sql.text(query), {"schema": schema})
            logger.debug("trino:fetch_cache_views")
            for record in res:
                self.cache_viewss[record.table_name]=dict(
                    name=record.table_name
                    )
            print("trino:insert each view in cache:->",cache_views[record.table_name])
            with open(filepath, "w") as file:
                json.dump(self.cache_views, file)

    def get_view_names(self, connection: Connection, schema: str = None, **kw) -> List[str]:
        schema = schema or self._get_default_schema_name(connection)
        if schema is None:
            raise exc.NoSuchTableError("schema is required")
        if self.prefetch_ddl:
            if self.cache_views=={}:
                self.cache_load_views(connection, schema)
            views = []
            for record in self.cache_views[schema].values():
                print("trino:find cache view:",record)
                view = record
                views.append(view)   
            print("trino:views=",views)
            return [view.name for view in views]
        else:
            # Querying the information_schema.views table is subpar as it compiles the view definitions.
            query = dedent(
                """
                SELECT "table_name"
                FROM "information_schema"."tables"
                WHERE "table_schema" = :schema
                AND "table_type" = 'VIEW'
            """
            ).strip()
            res = connection.execute(sql.text(query), {"schema": schema})
            return [row.table_name for row in res]

    def get_temp_view_names(self, connection: Connection, schema: str = None, **kw) -> List[str]:
        """Trino has no support for temporary views. Returns an empty list."""
        return []

    def cache_load_view_definitions(self, connection: Connection, schema: str = None):
        filepath=os.path.abspath("cache_view_definitions.json")
        if (os.path.isfile(filepath) and not is_file_older_than_x_days(filepath, self.ttl_days)):
            with open(filepath, "r") as file:
                print("trino:load cache view definitions from file:",filepath)
                self.cache_view_definitions = json.load(file)
        else:
            query = dedent(
                """
                SELECT "table_name","view_definition"
                FROM "information_schema"."views"
                WHERE "table_schema" = :schema
            """
            ).strip()
            res = connection.execute(sql.text(query), {"schema": schema})
            logger.debug("trino:fetch_cache_view_definitions")
            for record in res:
                sn=schema+":"+record.table_name
                if sn not in self.cache_view_definitons:
                    self.cache_view_definitions[sn]={}
                self.cache_view_definitions[sn][record.table_name]=dict(
                    view_definition=record.view_definition
                    )
                print("trino:insert each view_definition in cache:",sn,":->",self.cache_view_definitions[sn])
            with open(filepath, "w") as file:
                json.dump(self.cache_view_definitions, file)

    def get_view_definition(self, connection: Connection, view_name: str, schema: str = None, **kw) -> str:
        schema = schema or self._get_default_schema_name(connection)
        if schema is None:
            raise exc.NoSuchTableError("schema is required")
        if self.prefetch_ddl:
            if self.cache_view_definitons=={}:
                self.cache_load_view_definitions(connection, schema)
            view_definitions = []
            for record in self.cache_view_definitions[schema+":"+view_name].values():
                print("trino:find cache view_definition:",schema,":",view_name,":",record)
                view_definition = record
                view_definitions.append(view_definition)   
            print("trino:view_definitions=",view_definitions,":",view_definition)
            return view_definition
        else:
            query = dedent(
                """
                SELECT "view_definition"
                FROM "information_schema"."views"
                WHERE "table_schema" = :schema
                AND "table_name" = :view
            """
            ).strip()
            res = connection.execute(sql.text(query), {"schema": schema, "view": view_name})
            return res.scalar()

    def get_indexes(self, connection: Connection, table_name: str, schema: str = None, **kw) -> List[Dict[str, Any]]:
        if not self.has_table(connection, table_name, schema):
            raise exc.NoSuchTableError(f"schema={schema}, table={table_name}")

        partitioned_columns = None
        try:
            partitioned_columns = self._get_columns(connection, f"{table_name}$partitions", schema, **kw)
        except Exception as e:
            # e.g. it's not a Hive table or an unpartitioned Hive table
            logger.debug("Couldn't fetch partition columns. schema: %s, table: %s, error: %s", schema, table_name, e)
        if not partitioned_columns:
            return []
        partition_index = dict(
            name="partition",
            column_names=[col["name"] for col in partitioned_columns],
            unique=False
        )
        return [partition_index]

    def get_sequence_names(self, connection: Connection, schema: str = None, **kw) -> List[str]:
        """Trino has no support for sequences. Returns an empty list."""
        return []

    def get_unique_constraints(
        self, connection: Connection, table_name: str, schema: str = None, **kw
    ) -> List[Dict[str, Any]]:
        """Trino has no support for unique constraints. Returns an empty list."""
        return []

    def get_check_constraints(
        self, connection: Connection, table_name: str, schema: str = None, **kw
    ) -> List[Dict[str, Any]]:
        """Trino has no support for check constraints. Returns an empty list."""
        return []

    def cache_load_table_comments(self, connection: Connection, catalog_name: str = None, schema: str = None):
        filepath=os.path.abspath("cache_table_comments.json")
        if (os.path.isfile(filepath) and not is_file_older_than_x_days(filepath, self.ttl_days)):
            with open(filepath, "r") as file:
                print("trino:load cache table comments from file:",filepath)
                self.cache_table_comments = json.load(file)
        else:
            query = dedent(
                    """
                        SELECT "table_name","comment"
                        FROM "system"."metadata"."table_comments"
                        WHERE "catalog_name" = :catalog_name
                        AND "schema_name" = :schema_name
                    """
            ).strip()
            res = connection.execute(sql.text(query), {"schema_name": schema, "catalog_name": catalog_name})
            logger.debug("trino:fetch_cache_table_comments")
            for record in res:
                sn=schema+":"+record.table_name
                if sn not in self.cache_table_comments:
                    self.cache_table_comments[sn]={}
                self.cache_table_comments[sn][record.table_name]=dict(
                    comment=record.comment
                    )
                print("trino:insert each table_comment in cache:",sn,":->",self.cache_table_comments[sn])
            with open(filepath, "w") as file:
                json.dump(self.cache_table_comments, file)

    def get_table_comment(self, connection: Connection, table_name: str, schema: str = None, **kw) -> Dict[str, Any]:
        catalog_name = self._get_default_catalog_name(connection)
        if catalog_name is None:
            raise exc.NoSuchTableError("catalog is required in connection")
        schema_name = schema or self._get_default_schema_name(connection)
        if schema_name is None:
            raise exc.NoSuchTableError("schema is required")
        if self.prefetch_ddl:
            if self.cache_table_comments=={}:
                self.cache_load_table_comments(connection, catalog_name, schema)
            record=self.cache_table_comments[schema+":"+table_name].values()
            print("trino:find cache table_comments:",schema,":",table_name,":",record)
            comment = record
            print("trino:table_comments:",comment," from ",record)
            return dict(text=comment)
        else:
            query = dedent(
                    """
                    SELECT "comment"
                    FROM "system"."metadata"."table_comments"
                    WHERE "catalog_name" = :catalog_name
                    AND "schema_name" = :schema_name
                    AND "table_name" = :table_name
                """
                ).strip()
            try:
                res = connection.execute(
                    sql.text(query),
                    {"catalog_name": catalog_name, "schema_name": schema_name, "table_name": table_name}
                )
                return dict(text=res.scalar())
            except error.TrinoQueryError as e:
                if e.error_name in (
                    error.PERMISSION_DENIED,
                ):
                    return dict(text=None)
                raise

    def has_schema(self, connection: Connection, schema: str) -> bool:
        if self.prefetch_ddl:
            if self.cache_schemas=={}:
                self.cache_load_schemas(connection)
            return(self.cache_schema[schema] is not None)
        else:
            query = dedent(
                """
                SELECT "schema_name"
                FROM "information_schema"."schemata"
                WHERE "schema_name" = :schema
            """
            ).strip()
            res = connection.execute(sql.text(query), {"schema": schema})
            return res.first() is not None

    def has_table(self, connection: Connection, table_name: str, schema: str = None, **kw) -> bool:
        schema = schema or self._get_default_schema_name(connection)
        if schema is None:
            return False
        if self.prefetch_ddl:
            if self.cache_tables=={}:
                self.cache_load_tables(connection, schema)
            return(table_name in self.cache_tables[schema])
        else:
            query = dedent(
                """
                SELECT "table_name"
                FROM "information_schema"."tables"
                WHERE "table_schema" = :schema
                AND "table_name" = :table
            """
            ).strip()
            res = connection.execute(sql.text(query), {"schema": schema, "table": table_name})
            return res.first() is not None

    def has_sequence(self, connection: Connection, sequence_name: str, schema: str = None, **kw) -> bool:
        """Trino has no support for sequence. Returns False indicate that given sequence does not exists."""
        return False

    @classmethod
    def _get_server_version_info(cls, connection: Connection) -> Any:
        def get_server_version_info(_):
            query = "SELECT version()"
            try:
                res = connection.execute(sql.text(query))
                version = res.scalar()
                return tuple([version])
            except exc.ProgrammingError as e:
                logger.debug(f"Failed to get server version: {e.orig.message}")
                return None

        # Make server_version_info lazy in order to only make HTTP calls if user explicitly requests it.
        cls.server_version_info = property(get_server_version_info, lambda instance, value: None)

    def _raw_connection(self, connection: Union[Engine, Connection]) -> trino_dbapi.Connection:
        if isinstance(connection, Engine):
            return connection.raw_connection()
        return connection.connection

    def _get_default_catalog_name(self, connection: Connection) -> Optional[str]:
        dbapi_connection: trino_dbapi.Connection = self._raw_connection(connection)
        return dbapi_connection.catalog

    def _get_default_schema_name(self, connection: Connection) -> Optional[str]:
        dbapi_connection: trino_dbapi.Connection = self._raw_connection(connection)
        return dbapi_connection.schema

    def do_execute(
        self, cursor: Cursor, statement: str, parameters: Tuple[Any, ...], context: DefaultExecutionContext = None
    ):
        cursor.execute(statement, parameters)

    def do_rollback(self, dbapi_connection: trino_dbapi.Connection):
        if dbapi_connection.transaction is not None:
            dbapi_connection.rollback()

    def set_isolation_level(self, dbapi_conn: trino_dbapi.Connection, level: str) -> None:
        dbapi_conn._isolation_level = trino_dbapi.IsolationLevel[level]

    def get_isolation_level(self, dbapi_conn: trino_dbapi.Connection) -> str:
        return dbapi_conn.isolation_level.name

    def get_default_isolation_level(self, dbapi_conn: trino_dbapi.Connection) -> str:
        return trino_dbapi.IsolationLevel.AUTOCOMMIT.name

    def _get_full_table(self, table_name: str, schema: str = None, quote: bool = True) -> str:
        table_part = self.identifier_preparer.quote_identifier(table_name) if quote else table_name
        if schema:
            schema_part = self.identifier_preparer.quote_identifier(schema) if quote else schema
            return f"{schema_part}.{table_part}"

        return table_part
